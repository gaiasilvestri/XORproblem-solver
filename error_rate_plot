% Gaia Silvestri
% Neural network: XOR problem solver
% with a 2-element input layer, 4-element hidden layer, a 1-element output layer
% Trained with the delta rule / used a permuted sequence of training trials (the four possible inputs in various random orders)
% Plots error rate at across iterations
% November 2013

npass=10000;
nhidden=4;                % set the number of hidden units
W1=zeros(nhidden,2);      % synapses from the 2 input units to the 4 hidden units (tot of 8)
W2=zeros(nhidden,1);      % synapses from the 4 hidden units to the output unit (tot of 4)
zout=zeros(1,4);          % four outputs corresponding to the four inputs 
inp=[0 0; 0 1; 1 0; 1 1]; % the four possible inputs 
target1=[0 1 1 0];        % target for input as above
target=[0 0 0 0];         % set to zeros now, it will be filled with correct targets corresponding to the input pattern selection 
                          % for each pass
maxerr=zeros(1,npass);

dW2=zeros(nhidden,1);     % synaptic weight change for synapses between hidden and output layer
dW1=zeros(nhidden,2);     % synaptic weight change for synapses between input and hidden layer


a=0.5;                    %learning rate (0<a<1)
b=0.05;
alpha=0.7;      
m=0.8;                    %momentum (how much a previous weight change affects current weight change)
t=0.1;                    %tolerance (criterion for accepting an output as ok)
hbias=zeros(nhidden,1);   %hidden units bias
obias=0;                  %output units bias

%W1= [1 0; 0 0; 1 0; 0 1]; %for testing
%W2= [1; 0; 0; 1]; % for testing

% ASSIGN SYNAPTIC WEIGHTS: negative and positive weights between -1 and 1 

%weights between input layer and hidden layer: W1
for i=1:nhidden
    for k=1:2
    W1(i,k)=rand;           % comment out for non-random weights
    r=rand;
    if r<0.5
    W1(i,k)=W1(i,k)*-1;
    end
    end
end

% weights between hidden layer and output layer: W2
for i=1:nhidden
    r=rand;
    W2(i,1)=rand;           % comment out for non-random weights
    if r<0.5
    W2(i,1)=W2(i,1)*-1;
    end
end

%LEARNING FORWARD PASS
iterations=0;
for l=1:npass
    pindex=randperm(4);                         %rescramble patterns at every pass
    target=[0 0 0 0];                           %reset target output for each pass
    
        for s=1:4                               %each pass contains all 4 possible inputs in a random order
            curpat=inp(pindex(1,s),:);                      %get the four inputs in a random order
            target(1,s)=target1(1,pindex(1,s));             %get correct target output for the inputs in this current order
            %curpat=[0 1];  %for testing
            %y=curpat.*W1'; %for testing 
    %COMPUTE ACTIVATION OF EACH HIDDEN NEURON FOR INPUT PATTERN SELECTED
            y =bsxfun(@times,curpat,W1);                    %multiply: activation of each input neuron and its two outgoing weights to hidden layer
            y=sum(y,2);                                     %get summed input for each hidden neuron 
            for g=1:nhidden
            y(nhidden,1)=y(nhidden,1)+hbias(nhidden,1);     %add bias to each hidden neuron
        end
        
    yout=1./(1+exp((-1*(y-a))./b));      %squashes activation of each hidden neuron
    yout=yout';
    
    %COMPUTE ACTIVATION OF OUTPUT NEURON
    z(1,s)=yout*W2;         %multiply: activation of each hidden neuron and its outgoing weight to output layer               
    z(1,s)=z(1,s)+obias;                           %add obias to output neuron
    zout(1,s)=1./(1+exp((-1*(z(1,s)-a))./b));      %vector is filled when you did 4 patterns, then compare to comparison vector
    
    %BACKPROPAGATION OF ERROR!
     
    %COMPUTE OUTPUT LAYER ERROR given current pattern:
    %error = activation of output unit*(1-activation of output unit)*(target-activation of output unit)
    currentzout=zout(1,s);
    d=currentzout*(1-currentzout)*(target(1,s)-currentzout);
        
    %COMPUTE HIDDEN LAYER ERROR (for every hidden unit) given current pattern:
    %for each hidden unit:error = activation*(1-activation)*weight leaving that neuron*d
        for c=1:nhidden
            e(1,c)= yout(1,c)*(1-yout(1,c))*W2(c,1)*d;
        end
    %CALCULATE CHANGES TO THE WEIGHTS FOR THE HIDDEN->OUTPUT SYNAPSES in W2
        for v=1:nhidden
        dW2(v,1)= alpha*yout(1,v)*d + m*dW2(v,1);
        end
    %ADJUST WEIGHTS FOR THE HIDDEN->OUTPUT SYNAPSES in W2
        W2=dW2+W2;
        %adjust obias
        obias=obias+alpha*d;
    %CALCULATE CHANGES TO THE WEIGHTS FOR THE INPUT->HIDDEN SYNAPSES in W1
        for x=1:2
            for q=1:nhidden
                dW1(q,x)= alpha*curpat(1,x)*e(1,q)+m*dW1(q,x);
            end
        end
    %ADJUST WEIGHTS FOR THE INPUT->HIDDEN SYNAPSES in W1
        W1=dW1+W1;
        %adjust hbias
        for p=1:nhidden
        hbias(p,1)=hbias(p,1)+alpha*e(1,p);
        end
    end
    maxerr(1,l)=max(abs(target-zout));            %calculate error rate
    
    %IS THE OUTPUT FOR EACH INPUT PATTERN CORRECT?
    %Target-output < tolerance? for EACH INPUT PATTERN, break only if all 4 patterns pass this check
    %tss=(sum(se)); %total sum-squared error is the sum of all se (squared error for current training pattern)
    %hold on;
    %plot(tss);
    
        %are all outputs correct, based on our tolerance criterium?
        if abs(target(1,1)-zout(1,1))<t && abs(target(1,2)-zout(1,2))<t && abs(target(1,3)-zout(1,3))<t && abs(target(1,4)-zout(1,4))<t
           disp('target achieved within tolerance');
           disp(iterations);                         
           figure
           subplot(2,1,1);
           bar(W1);
           title('Subplot 1: W1 weights histogram');
           subplot(2,1,2);
           bar(W2);
           title('Subplot 2: W2 weights histogram');
           break
        end
       % hold off;
      iterations=iterations+1;    

end
%plot error rate across iterations
plot(maxerr);
